{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02 - Handling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will learn how to manipulate and handle data for analysis in Python. This will lay the foundations for any kind of statistical analysis or data visualizations we would like to build from social data.\n",
    "\n",
    "Before we begin, you should download this lab and follow along within the notebook.\n",
    "\n",
    "You can do this by clicking the \"Clone or download\" link on the GitHub page. Then, from within the Jupyter Notebook file navigator, go to the folder where you downloaded the lab and click the \"Lab 02 - Handling Data.ipynb\" file. \n",
    "\n",
    "![](img/download.png)\n",
    "\n",
    "## Data Frames\n",
    "\n",
    "When we undertake data analysis within social science, we tend to think of datasets which take on a *rectangular* format. Or, in more common terms, in a spreadsheet.\n",
    "\n",
    "For instance, this is a spreadsheet of MP candidates in the 42nd Canadian national election.\n",
    "\n",
    "![](img/data-frame.png)\n",
    "\n",
    "In typical data analysis, we call the spreadsheet a *data frame*. This is going to be usually be our basic object of analysis for social data. \n",
    "\n",
    "A data frame consists of *rows* and *columns*. Each row is an observation in the data and our unit of analysis. In the case of the data frame above, each row represents an MP candidate in the election. So for instance, the highlighted row in this data frame is the candidate, Lorraine E. Barnett, who ran as a Conservative candidate in the Avalon district of Newfoundland and Labrador (she lost).\n",
    "\n",
    "![](img/data-frame-row.png)\n",
    "\n",
    "Each column is some attribute, characteristic, or variable about each unit of analysis. For the candidates, this includes their \"Political Affiliation\", the French name of the party \"Appartenance politique\", the Candidate's Family and First name, etc.\n",
    "\n",
    "![](img/data-frame-column.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Frames in Python\n",
    "\n",
    "The main machinery for handling data frames and associated data structures in Python are a set of modules called <code>NumPy</code> and <code>pandas</code>. Usually, these modules are imported using the <code>import</code> function. By convention, these are called <code>np</code> and <code>pd</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create data frame in Python, we use the <code>DataFrame</code> *constructor*. A constructor is a special kind of method which creates a new instantiation of a Python object.\n",
    "\n",
    "The constructor is loaded from the pandas module, so we begin it with a <code>pd</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an empty dataframe\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates an empty DataFrame, which is not of much use to us. We can load data into a data frame in a few ways. We can pass a list of tuples to the constructor. Or we could pass a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_tuples = pd.DataFrame([(170, 100), (150, 80), (200, 130)])\n",
    "df_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_dict = pd.DataFrame([{'Height': 170, 'Weight': 100}, \n",
    "                        {'Height': 150, 'Weight': 80}, \n",
    "                        {'Height': 200, 'Weight': 130}])\n",
    "df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note a few things: typing the name of the data frame in Jupyter Notebook will display the data frame as a table for convenient viewing. If we pass a list of tuples of the <code>DataFrame</code> constructor, they won't have names by default. We do not have to do that when we pass a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# adding columns argument labels the columns\n",
    "df_tuples = pd.DataFrame([(170, 100), (150, 80), (200, 130)], columns = ['Height', 'Weight'])\n",
    "df_tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, however, we won't be passing lists of tuples or dictionaries to create a data frame. We will be loading in the data from a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and writing files\n",
    "\n",
    "The pandas module can load many different kinds of files, including those from Microsoft Excel, and other statistical packages such as Stata or SPSS. The format which we are going to rely on, however, is called Comma-Separated Values, or CSV. CSV files are spreadsheets which are, very simply divided by commas and new lines.\n",
    "\n",
    "For this lab, we are using [data](http://maps-cartes.ec.gc.ca/indicators-indicateurs/TableView.aspx?ID=1&lang=en) from Environment Canada which measures greenhouse emissions from large facilities. This file is originally in Excel format but I have converted it to CSV to demonstrate the format.\n",
    "\n",
    "These are the first two lines of the CSV file."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Facility ID,Organisation Name,Facility Name,Province,City,Address,Postal Code,NPRI ID,2014 GHG Emissions (kilotonnes of carbon dioxide equivalent),Facility GHG Data Link,NAICS Name,NAICS Data Link,Latitude,Longitude\n",
    "1,Produits forestiers R?solu,Division Alma,Quebec,Alma,1100 Melanion Street,G8B5W2,983,84.38,http://www.ec.gc.ca/ges-ghg/donnees-data/index.cfm?do=facility_info&amp;lang=En&amp;year=2014&amp;ghg_id=G10001,Mechanical Pulp Mills,http://www23.statcan.gc.ca/imdb/p3VD.pl?Function=getVDDetail&amp;db=imdb&amp;dis=2&amp;adm=8&amp;TVD=118464&amp;CVD=118471&amp;CST=01012012&amp;MLV=5&amp;CLV=5&amp;CHVD=&amp;CPV=322111,48.56335,-71.65609"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line is all the column names. Each of them is separated by a comma. The second line is the first data record. We can see in the Facility Name column, there's a comma in the value itself. Therefore, CSV will put the value in \"quotes\".\n",
    "\n",
    "You can load the file using the <code>read_csv</code> method. This method has a lot of different arguments to handle various types of files. You can learn these over time by looking the [pandas documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html). For now, we will load the file without many options. The <code>latin1</code> option tells <code>pandas</code> to open the file in a certain file encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg = pd.read_csv(\"data/Greenhouse_Gas_Emissions.csv\", encoding = \"latin1\")\n",
    "df_gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typing the data frame in Jupyter Notebook will print out the first 30 and the last 30 lines to avoid overloading your output.\n",
    "\n",
    "Something to note is that the first column of the data frame is not the first column in the file. The first column in the file is <code>Facility ID</code>. But the first column in the data frame is an unlabeled column. This is called the *index*. The index can be defined by us or it can be defined by pandas. If it isn't defined by us, pandas assigns an index which goes from 0 to n-1, where n is the number of rows in the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we get a file, we probably want to understand some different parts of it. The key variable in this file is greenhouse gas (GHG) emissions. There's also information on province of the facility, city, address, postal code, and latitude/longitude.\n",
    "\n",
    "So some basic ways we can learn about the data are to learn about the data frame itself and properties of particular columns.\n",
    "\n",
    "## Describing the DataFrame\n",
    "\n",
    "We can look at the <code>columns</code> list of the data frame to look at which columns are in the data. This is a little prettier than the <code>head</code> command above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the <code>shape</code> tuple which tells us how many rows and columns are in the data frame. By convention, this tuple is in the form *(n, m)*, where *n* is the number of rows, and *m* is the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use <code>dtypes</code> to understand the type of columns in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the columns are of type <code>object</code>, which typically means they are strings. Facility ID is an int, while four other columns are floats. They have a 64 at the end of them, which means they can be up to 64 bits long. That's not important to know. You should focus on the type instead. Once you know the type, this can help you describe the individual columns.\n",
    "\n",
    "Lastly, you can use the <code>head()</code> and <code>tail()</code> methods to see the beginning and end of the file. You can pass a number as an optional argument to see several lines. By default, it will display 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing columns\n",
    "\n",
    "We generally want to try to detect some patterns in the data, and for that we need to analyze some properties of the columns. Since GHG emissions is the main variable, we can do some basic statistical operations on that variable. We can get measures of central tendency, such as mean and median, and measures of dispersion, such as standard deviation and range.\n",
    "\n",
    "We can access particular columns by typing the name of the data frame and putting the column name in quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## measures of central tendency\n",
    "df_gg['2014 GHG Emissions (kilotonnes of carbon dioxide equivalent)'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg['2014 GHG Emissions (kilotonnes of carbon dioxide equivalent)'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## measures of dispersion\n",
    "df_gg['2014 GHG Emissions (kilotonnes of carbon dioxide equivalent)'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg['2014 GHG Emissions (kilotonnes of carbon dioxide equivalent)'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg['2014 GHG Emissions (kilotonnes of carbon dioxide equivalent)'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we are dealing with non-numerical data. Most of the columns in this data frame are in fact non-numeric (in statistics we would call them *categorical* or *ordinal* variables). These contain important information about the facility.\n",
    "\n",
    "Say we want to know which province is most represented here. We can use the <code>value_counts()</code> method on the *Province* column to learn this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg['Province'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alberta has the highest proportion of facilities with a large amount of GHG emissions. Which seems to make sense, since Alberta processes a great deal of fossil fuels.\n",
    "\n",
    "If we just want to know which provinces are in the dataset, we could use the <code>unique()</code> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg['Province'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "The [iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) measures several properties of different iris flowers of  different species. We can use it here because it is an easy way to demonstrate pandas operations. For this exercise, do the following:\n",
    "\n",
    "1. Load the iris dataset. It is located at 'data/iris.csv'.\n",
    "2. Print the data frame. Display its dimensions and its types.\n",
    "3. Take the mean and median of sepal length.\n",
    "4. Get the min and max of petal width.\n",
    "5. Count how many times each species occurs in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding and removing columns\n",
    "\n",
    "Once we have the data frame, we can change it in important ways. We can add new columns to store new information or we can rename columns.\n",
    "\n",
    "The GHG emissions column has a really long name. Let's just call it GHG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_gg['GHG'] = df_gg['2014 GHG Emissions (kilotonnes of carbon dioxide equivalent)']\n",
    "del df_gg['2014 GHG Emissions (kilotonnes of carbon dioxide equivalent)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does two things. It assigns the values in the original column to a new one. Then, it deletes the original column with the <code>del</code> operator. This will also change the order of the columns because pandas will add GHG to the end. You can confirm this by yourself by typing <code>df_gg.columns</code> again.\n",
    "\n",
    "You can set new entire columns by assigning a value to them. Let's say we want to add a country column. These are all in Canada, so we can just set them to Canada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_gg['Country'] = 'Canada'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't terribly useful here, but once we show how to take slices of data, it is much more powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing and indexing\n",
    "\n",
    "One of the big reasons that pandas was created is because Python needed a way to index and access data by names and to use those indexes to perform multiple operations at once. This is where slicing and indexing comes in. This is both the most powerful (and often most frustrating) parts of pandas.\n",
    "\n",
    "For instance, say I just want to look at GHG emissions within Ontario. I can subset the data by using a conditional statement that matches for instances of Ontario in the province variable. I can also assign that to a new data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg_ON = df_gg[df_gg['Province'] == 'Ontario']\n",
    "df_gg_ON.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg_ON.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also index by numerical conditions. Let's get all the facilities which have GHG emissions higher than the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg_top50percent = df_gg[df_gg['GHG'] > df_gg['GHG'].median()]\n",
    "df_gg_top50percent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg_top50percent.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can combine different conditions with logical operators <code>&</code> and <code>|</code>. <code>&</code> (called an ampersand) is an operator referred to as \"bitwise and\", while <code>|</code> (called a pipe) is an operator referred to as \"bitwise or\". \n",
    "\n",
    "We also need to wrap each condition in parenthesis.\n",
    "\n",
    "Let's get those which are in the top 50 percent and also are in Ontario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg_top50percent_ON = df_gg[(df_gg['GHG'] > df_gg['GHG'].median()) & (df_gg['Province'] == 'Ontario')]\n",
    "df_gg_top50percent_ON.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg_top50percent_ON.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is probably the most important way you're going to be splitting data within pandas. You can also get different data by position in the table. Let's say I just want to get the first two rows of the file. We can use a syntax which is the same as list slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar manner, we can also select on several columns. Say we only want to know the facility name, the province, its city, and its GHG value. We can index the data frame by a list of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg[['Facility Name', 'City', 'Province', 'GHG']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating data\n",
    "\n",
    "Another nifty thing about pandas is that we can manipulate data across columns, rows, and even multiple columns and rows, at the same time.\n",
    "\n",
    "Say we want to convert the GHG variable from kilotonnes to just tonnes. That would mean we multiple the column by 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg['GHG_tonnes'] = df_gg['GHG'] * 1000\n",
    "df_gg[['GHG', 'GHG_tonnes']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the GHG variable (and any numeric variable) in most kinds of arithmetic operations we could like.\n",
    "\n",
    "For string columns, there's a set of methods we can use with those columns. Many of them are similar to the ones we saw last week. The full list is [here](http://pandas.pydata.org/pandas-docs/stable/text.html).\n",
    "\n",
    "Let's say that Toronto officially its name to \"DrakeLand\" tomorrow. We can use <code>contains</code> as an index to find all instances where the City column matches Toronto. **Notice** how after accessing the City column, we use <code>str</code> to note that this is a string operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg[df_gg['City'].str.contains('Toronto')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can replace all instances of \"Toronto\" with \"DrakeLand\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_gg['City'] = df_gg['City'].str.replace('Toronto', 'DrakeLand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This should not display any rows.\n",
    "df_gg[df_gg['City'].str.contains('Toronto')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Best I Ever Had\n",
    "df_gg[df_gg['City'].str.contains('DrakeLand')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do things like capitalize or lowercase strings. Let's do the province names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gg['Province'] = df_gg['Province'].str.lower()\n",
    "df_gg['Province'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Let's convert them back\n",
    "df_gg['Province'] = df_gg['Province'].str.capitalize()\n",
    "df_gg['Province'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## We can use title to capitalize the beginning of each word\n",
    "df_gg['Province'] = df_gg['Province'].str.title()\n",
    "df_gg['Province'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "1. Go back to the Iris dataset. Select only the rows which have a sepal length larger than the mean. Assign them to a new dataframe.\n",
    "2. Select flowers which 1) have a sepal width larger than the mean and 2) belong to the species setosa. Assign them to a new dataframe.\n",
    "3. Create a new data frame which only has two columns: petal length and petal width.\n",
    "4. Convert all numeric values to meters from centimeters.\n",
    "5. Change the name of all flowers with the species virginica to \"harmonica\"."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
